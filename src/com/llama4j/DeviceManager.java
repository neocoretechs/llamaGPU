package com.llama4j;

import java.util.concurrent.ConcurrentHashMap;

public final class DeviceManager {
	private static boolean DEBUG = false;
	public static enum TensorState {
		ON_DEVICE,
		ON_HOST
	}
	static class DeviceStatus {
		TensorState state;
		String id;
		long uploads = 0L;
		long downloads = 0L;
		long uploadBytes = 0L;
		long downloadBytes = 0L;
		DeviceStatus(TensorState state, String id) {
			this.state = state;
			this.id = id;
		}
		public String toString() {
			return String.format("Tensor %s uploads=%d downloads=%d upload bytes = %d download bytes = %d", id, uploads, downloads, uploadBytes, downloadBytes);
		}
	}
	static class DeviceTensorStatus extends DeviceStatus {
		DeviceTensor dt;
		DeviceTensorStatus(DeviceTensor dt, TensorState state, String id) {
			super(state, id);
			this.dt = dt;
		}
	}
	
	private static final ConcurrentHashMap<Long, DeviceStatus> deviceMap = new ConcurrentHashMap<>();
	private static final ConcurrentHashMap<String,DeviceTensorStatus> tensorsById = new ConcurrentHashMap<>();
	private static final ConcurrentHashMap<Long,DeviceTensorStatus> tensorsByPtr = new ConcurrentHashMap<>();

	/**
	 * Copy {@link FloatTensor} to device
	 * @param t
	 * @param id
	 * @param reupload
	 */
	public static synchronized void offer(FloatTensor t, String id, boolean reupload) {
		if(t.devicePtrOr0() == 0L) {
			t.allocDevice();
			deviceMap.put(t.devicePtrOr0(), new DeviceStatus(TensorState.ON_HOST, id));
		}
		DeviceStatus status = deviceMap.get(t.devicePtrOr0());
		if(status == null) { // allocated outside device manager, deal with it
			deviceMap.put(t.devicePtrOr0(), new DeviceStatus(TensorState.ON_HOST, id));
			System.out.println("WARNING - Tensor "+id+" allocated outside device manager.");
		} else {
			if(status.state != TensorState.ON_DEVICE) {
				if(t.isUploaded())
					System.out.println("WARNING - Tensor "+id+" uploaded outside device manager.");
				t.copyHostToDevice(id);
				status.state = TensorState.ON_DEVICE;
				status.uploadBytes += t.totalBytes();
				++status.uploads;
			} else {
				if(!t.isUploaded() || reupload) { // might have been marked dirty by setModified
					t.copyHostToDevice(id);
					status.uploadBytes += t.totalBytes();
					++status.uploads;
				}
			}
		}
	}
	
	/**
	 * Copy {@link DeviceTensor} pointer table to device. Locate by id in deviceMap via derived DeviceTensorStatus class.
	 * If we have to create it, as in it wasnt in the table via id, check and upload all associated FloatTensors via
	 * offer of FloatTensor.
	 * @param t
	 * @param id
	 * @param reupload
	 */
	public static DeviceTensor offer(FloatTensor[] t, String id, boolean reupload) {
	    DeviceTensorStatus status = tensorsById.get(id);
	    DeviceTensor dt;
	    if (status == null) {
	        dt = new DeviceTensor(t);
	        dt.allocDevice();
	        status = new DeviceTensorStatus(dt, TensorState.ON_HOST, id);
	        tensorsById.put(id, status);
	        if(tensorsByPtr.get(dt.devicePtrOr0()) != null)
	        	System.out.println("WARNING - mismatch between DeviceTensor tables!");
	        tensorsByPtr.put(dt.devicePtrOr0(), status);
	    } else {
	        dt = status.dt;
	        if (dt.devicePtrOr0() == 0L) {
	            dt.allocDevice();
	            tensorsByPtr.put(dt.devicePtrOr0(), status);
	        }
	    }
	    if (status.state != TensorState.ON_DEVICE || !dt.isUploaded() || reupload) {
	        dt.copyHostToDevicePointerTable(id);
	        status.state = TensorState.ON_DEVICE;
	        status.uploadBytes += dt.totalBytes();
	        ++status.uploads;
	    }
	    for (int i = 0; i < t.length; i++) {
	        offer(t[i], id + " element " + i, false);
	    }
	    if(DEBUG)
	    	for(int i = 0; i < dt.size(); i++)
	    		System.out.println("dt "+id+" "+i+"="+dt.getLong(i));
	    return dt;
	}

	/**
	 * called from FloatTensor.freeDevice() and {@link DeviceTensor#freeDevice}
	 * @param devicePtr
	 */
	public static void remove(long devicePtr) {
		deviceMap.remove(devicePtr);
		Object dst = tensorsByPtr.remove(devicePtr);
		if(dst != null)
			tensorsById.remove(((DeviceTensorStatus)dst).id);
	}

	/**
	 * Copy data from device to devicetensor pointer table, this should be rare if at all
	 * @param t
	 */
	public static synchronized void reclaim(DeviceTensor t, String id) {
		if(t.isImmutable()) {
			throw new RuntimeException("Attempt to reclaim immutable tensor "+id);
		}
		if(t.devicePtrOr0() == 0L) {
			throw new RuntimeException("DeviceTensor "+id+" was already freed. Cannot download from device.");
		}
		DeviceTensorStatus status = tensorsByPtr.get(t.devicePtrOr0());
		if(status == null) { // allocated outside device manager, deal with it
			status = new DeviceTensorStatus(t,TensorState.ON_HOST, id);
			tensorsByPtr.put(t.devicePtrOr0(), status);
			tensorsById.put(id, status);
			System.out.println("WARNING - DeviceTensor "+id+" was previously unknown to device manager.");
		} else {
			if(status.state != TensorState.ON_DEVICE) { // not on device for device manager, but uploaded to tensor?
				if(t.isUploaded()) {
					status.state = TensorState.ON_DEVICE;
					System.out.println("WARNING - Tensor "+id+" uploaded outside device manager.");
					t.copyDeviceToHostPointerTable(id);
					status.downloadBytes += t.totalBytes();
					++status.downloads;
				}
			} else { // its on device, download
				t.copyDeviceToHostPointerTable(id);
				status.downloadBytes += t.totalBytes();
				++status.downloads;
			}
		}
	}
	

	public static void report() {
	    for (DeviceStatus ds : deviceMap.values()) {
	        System.out.println(ds);
	    }
	}
	public static void reset() {
	    for (DeviceStatus ds : deviceMap.values()) {
	        ds.uploads = ds.downloads = 0L;
	        ds.uploadBytes = ds.downloadBytes = 0L;
	    }
	}
	
	
	static FloatTensor softmax(FloatTensor thiz, int thisOffset, int size) {
		try {
			offer(thiz, "softmax", false);
			Llama3.launchSoftmaxInplace.invokeExact(thiz.devicePtrOr0(), thisOffset, size);
			return thiz;
		} catch (Throwable e) {
			throw new RuntimeException(e);
		}
	}
	
	static void softmaxCpu(FloatTensor thiz, int thisOffset, int size) {
		//reclaimTest(thiz, "softmax");
		float maxVal = thiz.max(thisOffset, size);
		// exp and sum
		thiz.mapInPlace(thisOffset, size, f -> (float) Math.exp(f - maxVal));
		float sum = thiz.sum(thisOffset, size);
		// normalize
		thiz.divideInPlace(thisOffset, size, sum);
		//offer(thiz, "softmax", true);
	}
	

    
    static void rmsnormCpu(FloatTensor out, FloatTensor x, FloatTensor weight, int size, float eps) {
    	// calculate sum of squares
     	//reclaimTest(x, "rmsnorm x");
    	//reclaimTest(weight, "rmsnorm weight");
    	//reclaimTest(out, "rmsnorm out");
    	float ss = x.reduce(0, size, 0f, (acc, xi) -> acc + xi * xi);
    	ss /= size;
    	ss += eps;
    	ss = (float) (1.0 / Math.sqrt(ss));
    	// normalize and scale
    	final float finalss = ss; // for the lambda
    	out.mapWithIndexInPlace(0, size, (value, index) -> weight.getFloat(index) * (finalss * x.getFloat(index)));
      	//offer(x, "rmsnorm x", true); // re-upload
    	//offer(weight, "rmsnorm weight", true);
    	//offer(out, "rmsnorm out", true);
    }
    
    static void weightedSum(FloatTensor att, FloatTensor xb, FloatTensor vCache, int h, int headSize, int attOffset, int xbOffset, int kvDim, int kvMul, int position, int token) {
    	//void launch_weighted_sum(uint8_t* Att, uint8_t* xb, uint8_t* vCache, int h, int headSize, 
		// int attOffset, int xbOffset, int vcOffset, int kvDim, int kvMul, int position, int token, int size) 
    	try {
    		offer(att, "weightedSum att", false);
    		offer(xb, "weightedSum xb", false);
    		offer(vCache, "weightedSum vCache", false);
    		Llama3.launchAV.invokeExact(att.devicePtrOr0(), xb.devicePtrOr0(), vCache.devicePtrOr0(), 
    				h, headSize, attOffset, xbOffset, kvDim, kvMul, position, token);
    	} catch (Throwable e) {
    		throw new RuntimeException(e);
    	}
    }
    
    static void weightedSumCpu(FloatTensor att, FloatTensor xb, FloatTensor vCache, int h, int headSize, int attOffset, int xbOffset, int kvDim, int kvMul, int position, int token) {
    	//void launch_weighted_sum(uint8_t* Att, uint8_t* xb, uint8_t* vCache, int h, int headSize, 
		// int attOffset, int xbOffset, int vcOffset, int kvDim, int kvMul, int position, int token, int size) 
    	try {
    		//reclaimTest(att, "weightedSum att");
    		//reclaimTest(xb, "weightedSum xb");
    		//reclaimTest(vCache, "weightedSum vCache");
            // weighted sum of the values, store back into xb
            // float* xb = s.xb + h * headSize;
            //int xbOffset = h * headSize;
            // memset(xb, 0, headSize * sizeof(float));
            xb.fillInPlace(xbOffset, headSize, 0f);
            for (int t = 0; t <= position + token; t++) {
                // get the value vector for this head and at this timestep
                // float* v = s.value_cache + loff + t * dim + h * headSize;
                int vOffset = t * kvDim + (h / kvMul) * headSize;
                // get the attention weight for this timestep
                float a = att.getFloat(attOffset + t);
                // accumulate the weighted value into xb
                xb.saxpyInPlace(xbOffset, vCache, vOffset, headSize, a);
            }
     		//offer(att, "weightedSum att", true);
    		//offer(xb, "weightedSum xb", true);
    		//offer(vCache, "weightedSum vCache", true);
    	} catch (Throwable e) {
    		throw new RuntimeException(e);
    	}
    }
    
  
    static void qkScoresCpu(FloatTensor q, int qOffset, FloatTensor keyCache,  
    	    FloatTensor Att, int attOffset, int position, int token, int h, int headSize, int numHeads, int contextLength, int kvDim, int kvMul ) {
    		float sqrtHeadSize = (float) Math.sqrt(headSize);
    		//System.out.printf("qkscores= qOff=%d attOff=%d pos=%d, token=%d, h=%d, headsize=%d kvDim=%d kvMul=%d\n",qOffset, attOffset, position, token,h,headSize, kvDim, kvMul);
        	try {
        		//reclaimTest(q, "qkScore q");
        		//reclaimTest(keyCache, "qkScore keyCache");
        		//reclaimTest(Att, "qkScore Att");
    			//Llama3.launchQK.invokeExact(q.devicePtrOr0(), qOffset, q.getFormatType(), q.type().getBlockSize(), q.type().getTypeSize(), q.getHeadSize(),
    			//		keyCache.devicePtrOr0(), keyCache.getFormatType(), keyCache.type().getBlockSize(), keyCache.type().getTypeSize(), keyCache.getHeadSize(),
    			//		Att.devicePtrOr0(), attOffset, position, token, h, headSize, kvDim, kvMul);
                //int token = (int) (ht / config.numberOfHeads);
                //int h = (int) (ht % config.numberOfHeads);
                // get the query vector for this head
                // float* q = s.q + h * headSize;
                //int qOffset = h * headSize;
                // attention scores for this head
                // float* att = s.att + h * config.seq_len;
                //int attOffset = h * config.contextLength;
                // iterate over all timesteps, including the current one
                long nanos2 = System.nanoTime();
                for (int t = 0; t <= position + token; t++) {
                	// get the key vector for this head and at this timestep
                	// float* k = s.key_cache + loff + t * dim + h * headSize;
                	int keyCacheOffset = t * kvDim + (h / kvMul) * headSize;
                	// calculate the attention score as the dot product of q and k
                	//float score = q.dot(qOffset, keyCache, keyCacheOffset, headSize);
                	float score = FloatTensor.scalarDot(q, qOffset, keyCache, keyCacheOffset, headSize);
                	score /= sqrtHeadSize;
                	// save the score to the attention buffer
                	Att.setFloat(attOffset + t, score);
                }
                nanos2 = System.nanoTime() - nanos2;
                //softmax the scores to get attention weights, from 0..position inclusively
                Att.softmaxInPlace(attOffset, position + token + 1);
          		//offer(q, "qkScore q", true);
        		//offer(keyCache, "qkScore keyCache", true);
        		//offer(Att, "qkScore Att", true);
    		} catch (Throwable e) {
    			throw new RuntimeException(e);
    		}
	}
    
 
    static float sdotCpu(FloatTensor thiz, int thisOffset, FloatTensor that, int thatOffset, int size) {
    	float result = 0.0f;
		try {
			//reclaimTest(thiz, "sdot thiz");
			//reclaimTest(that, "sdot that");
			result = FloatTensor.scalarDot(thiz, thisOffset, that, thatOffset, size);
			//offer(thiz, "sdot thiz", true);
			//offer(that, "sdot that", true);
		} catch (Throwable e) {
			throw new RuntimeException(e);
		}
		return result;
    }
    
 
  
    protected static void printParallelMatmul() {
        /*System.out.println("Parallel matmul print start:");
        Parallel.parallelForLong(0, (long) nTokens * (long) config.numberOfHeads, ht -> {
            final int token = (int) (ht / config.numberOfHeads);
            final int h     = (int) (ht % config.numberOfHeads);
            // Offsets for this head
            final int qOffset   = h * headSize;
            final int attOffset = h * config.contextLength;
            // Time horizon for this token (inclusive of current position)
            final int T = position + token + 1;
            System.out.println(Thread.currentThread().getName()+"| (T, token, h, qOffset, attOffset) T="+T+" token="+token+" h="+h+" qOffset="+qOffset+" attOffset="+attOffset);
            for (int t = 0; t < T; t++) {
                final int keyCacheOffset = t * kvDim + (h / kvMul) * headSize;
                //System.out.println(Thread.currentThread().getName()+"|t loop (curLayer, keyCacheOffset, headSize) - float[] kVec = state.keyCache["+curLayer+"].exportSlicePooled(Llama3.poolHead,"+ keyCacheOffset+","+ headSize+")");
            }
            // 4) Weighted sum over values â†’ xb
            final int xbOffset = h * headSize;
            for (int t = 0; t < T; t++) {
                final int vOffset = t * kvDim + (h / kvMul) * headSize;
                //System.out.println(Thread.currentThread().getName()+"|t loop (token, attOffset, t) float a = state.att["+token+"].getFloat("+(attOffset + t)+")");
                //System.out.println(Thread.currentThread().getName()+"|t loop (token xbOffset, curLayer, vOffset, headSize) state.xb["+token+"].saxpyInPlace("+xbOffset+",state.valueCache["+curLayer+"],("+vOffset+","+ headSize+", a)");
            }
        });
        System.out.println("Parallel matmul print end");*/		
    }

}
